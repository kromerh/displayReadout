{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a synthetic dataset of picture that do not contain images\n",
    "\n",
    "- use the house numbers pictures, take each 32x32 frames in the corner of the images as synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the house model dataset, take 4 corners in images that are larger than some defined size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from SVHNDataset import SVHNDataset\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # otherwise there will be an error\n",
    "\n",
    "# list of the image files in the training set\n",
    "folder_images = '/Users/hkromer/02_PhD/02_Data/12.dcr/Stanford_housenumbers/train/'\n",
    "\n",
    "files_images = os.listdir(folder_images)\n",
    "files_images = [f\"{folder_images}{f}\" for f in files_images if f.endswith('png')]\n",
    "\n",
    "# load images\n",
    "images = []\n",
    "#4 corners\n",
    "framesize = 32\n",
    "# can be made a lot more efficient using numpy arrays\n",
    "images_corners = np.empty([4*len(files_images),framesize,framesize]) # it will be at most 4 times the size of the original images\n",
    "\n",
    "jj = 0 # index in images_corners\n",
    "for ii, file in enumerate(files_images):\n",
    "    img = cv2.imread(file, 0) #grayscale\n",
    "    # convert to grayscale\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "    # take only images that are significantly larger so that there will not be too much of the digit in the image\n",
    "    if (width > 50) & (height > 50):\n",
    "        images.append(img)\n",
    "        # extrat the 4 corners in each image\n",
    "\n",
    "        # left positions of the corner\n",
    "        c1 = img[0:framesize,0:framesize].copy() # top left\n",
    "        c2 = img[0:framesize,width-framesize:].copy() # top right\n",
    "        c3 = img[height-framesize:,0:framesize].copy() # bottom left\n",
    "        c4 = img[height-framesize:,width-framesize:].copy() # bottom right\n",
    "        images_corners[jj] = c1\n",
    "        jj += 1\n",
    "        images_corners[jj] = c2\n",
    "        jj += 1\n",
    "        images_corners[jj] = c3\n",
    "        jj += 1\n",
    "        images_corners[jj] = c4\n",
    "        jj += 1\n",
    "\n",
    "# not all the images satisfied the width and height criteria, adjust the images_corner array\n",
    "s = np.sum(np.sum(images_corners,axis=1),axis=1)\n",
    "s_limit = s[s>0].shape[0]\n",
    "\n",
    "images_corners = images_corners[0:s_limit,:,:]\n",
    "\n",
    "print(f'Loaded images in the corner, shape of dataset: {images_corners.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svhn = SVHNDataset()\n",
    "\n",
    "\n",
    "data_negative = images_corners.copy() # this will be y = 0 class\n",
    "\n",
    "# preprocessing for keras, because it wants image numbers, frame, frame, channels\n",
    "data_negative = data_negative.reshape(-1,32,32,1)\n",
    "\n",
    "print('After preprocessing reshaping: ')\n",
    "print(f'Negative dataset: {data_negative.shape}')\n",
    "\n",
    "\n",
    "# load the dataset with the house numbers from SVHN\n",
    "# these do all contain numbers, so that will be the positive dataset, y=1\n",
    "\n",
    "path_train  = '/Users/hkromer/02_PhD/02_Data/12.dcr/Stanford_housenumbers/train_32x32.mat'\n",
    "path_test  = '/Users/hkromer/02_PhD/02_Data/12.dcr/Stanford_housenumbers/test_32x32.mat'\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = svhn.load_dataset(path_train, path_test)\n",
    "# convert to grayscale\n",
    "train_data = svhn.convert_to_gray(train_data)\n",
    "test_data = svhn.convert_to_gray(test_data)\n",
    "print(' ')\n",
    "print('After conversion to grayscale: ')\n",
    "print(f'Original SVHN train data: {train_data.shape}, labels: {train_labels.shape}')\n",
    "print(f'Original SVHN test data: {test_data.shape}, labels: {test_labels.shape}')\n",
    "\n",
    "X_train = svhn.preprocess_for_KERAS_reshaping(32, train_data)\n",
    "X_test = svhn.preprocess_for_KERAS_reshaping(32, test_data)\n",
    "print(' ')\n",
    "print('After preprocessing reshaping: ')\n",
    "print(f'Original SVHN train data: {X_train.shape}')\n",
    "print(f'Original SVHN test data: {X_test.shape}')\n",
    "\n",
    "# combine the original test and train dataset into one\n",
    "data_positive = np.concatenate((X_train, X_test))\n",
    "\n",
    "print(' ')\n",
    "print(f'Positive dataset: {data_positive.shape}')\n",
    "\n",
    "# shuffle the positive and the negative dataset\n",
    "np.random.shuffle(data_positive)\n",
    "np.random.shuffle(data_negative)\n",
    "\n",
    "# make the two classes equal\n",
    "data_positive = data_positive[0:data_negative.shape[0],:,:,:]\n",
    "print(' ')\n",
    "print('Made two classes equal in size')\n",
    "print(f'Positive dataset: {data_positive.shape}')\n",
    "print(f'Negative dataset: {data_negative.shape}')\n",
    "\n",
    "labels_negative = np.zeros(data_negative.shape[0]) # y = 0\n",
    "labels_positive = np.ones(data_positive.shape[0]) # y = 0\n",
    "\n",
    "# join the two classes\n",
    "X = np.concatenate((data_positive,data_negative))\n",
    "y = np.concatenate((labels_positive,labels_negative))\n",
    "\n",
    "# shuffle\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "print(' ')\n",
    "print('Shuffled and joined the classes')\n",
    "print(f'Features X: {X.shape}, labels y: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80244 samples\n",
      "Epoch 1/10\n",
      "80244/80244 [==============================] - 130s 2ms/sample - loss: 0.2686 - accuracy: 0.9203\n",
      "Epoch 2/10\n",
      "80244/80244 [==============================] - 133s 2ms/sample - loss: 0.1412 - accuracy: 0.9487\n",
      "Epoch 3/10\n",
      "80244/80244 [==============================] - 134s 2ms/sample - loss: 0.1289 - accuracy: 0.9539\n",
      "Epoch 4/10\n",
      "80244/80244 [==============================] - 133s 2ms/sample - loss: 0.1153 - accuracy: 0.9577\n",
      "Epoch 5/10\n",
      "80244/80244 [==============================] - 135s 2ms/sample - loss: 0.1094 - accuracy: 0.9605\n",
      "Epoch 6/10\n",
      "80244/80244 [==============================] - 138s 2ms/sample - loss: 0.1002 - accuracy: 0.9637\n",
      "Epoch 7/10\n",
      "80244/80244 [==============================] - 139s 2ms/sample - loss: 0.0959 - accuracy: 0.9662\n",
      "Epoch 8/10\n",
      "80244/80244 [==============================] - 137s 2ms/sample - loss: 0.0918 - accuracy: 0.9673\n",
      "Epoch 9/10\n",
      "80244/80244 [==============================] - 137s 2ms/sample - loss: 0.0921 - accuracy: 0.9678\n",
      "Epoch 10/10\n",
      "80244/80244 [==============================] - 137s 2ms/sample - loss: 0.0914 - accuracy: 0.9680\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,1)),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation='softmax') # either 1 or 0\n",
    "])\n",
    "\n",
    "# define the optimizer, loss function and metrics for the network\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# start training\n",
    "history = model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # otherwise there will be an error\n",
    "\n",
    "model.save('/Users/hkromer/02_PhD/02_Data/12.dcr/Stanford_housenumbers/2019-19-25.KERAS_model_DigitDetector.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(\"Model accuracy on test data is: {:6.3f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # otherwise there will be an error\n",
    "path = '/Users/hkromer/02_PhD/02_Data/12.dcr/Stanford_housenumbers/2019-19-25.KERAS_model_DigitDetector.h5'\n",
    "\n",
    "model = svhn.load_model(path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = 9\n",
    "examples = X_test[ii,:,:]\n",
    "examples = examples.astype(np.float64)\n",
    "examples = examples.reshape(-1,32,32,1)\n",
    "y_predict = model.predict(examples)[0]\n",
    "y_example = y_test[ii]\n",
    "X = [a for a in range(0,len(y_predict))]\n",
    "\n",
    "plt.bar(X,y_predict)\n",
    "plt.xticks(np.arange(0,10,1))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "examples=examples.reshape(32,32)\n",
    "plt.imshow(examples,cmap='gray')\n",
    "plt.show()\n",
    "print(y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
